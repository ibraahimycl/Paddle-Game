# ğŸ“ Paddle Game with Enhanced AI

Bu proje, derin Q-Ã¶ÄŸrenme (Deep Q-Learning) kullanarak paddle oyunu iÃ§in geliÅŸmiÅŸ bir AI sistemi iÃ§erir.

## ğŸš€ Ã–zellikler

### ğŸ¤– AI Modelleri
- **DQN AI** (`dqn_ai.py`): Derin Q-Network ile geliÅŸmiÅŸ AI
- **Simple AI** (`paddle_ai.py`): Basit Q-Learning tabanlÄ± AI

### ğŸ§  GeliÅŸmiÅŸ AI Ã–zellikleri

#### 1. **Enhanced State Vector**
- **Ã–nceki**: 4 boyutlu state (ball_y, paddle_y, ball_direction_x, ball_direction_y)
- **Yeni**: 6 boyutlu state (ball_y, paddle_y, ball_direction_x, ball_direction_y, **ball_velocity_x**, **paddle_height**)
- **Normalizasyon**: TÃ¼m deÄŸerler ekran boyutlarÄ±na gÃ¶re normalize edilir

#### 2. **Double DQN Implementation**
```python
# Main network ile en iyi aksiyonu seÃ§
best_actions = torch.argmax(self.model(next_states), dim=1)
# Target network ile Q deÄŸerini hesapla
next_q_values = self.target_model(next_states).gather(1, best_actions.unsqueeze(1))
```

#### 3. **Target Network Updates**
- Her 100 adÄ±mda bir target network gÃ¼ncellenir
- Daha stabil eÄŸitim saÄŸlar

#### 4. **Logarithmic Epsilon Decay**
```python
self.epsilon = self.epsilon_min + (1.0 - self.epsilon_min) * math.exp(-decay_rate * self.steps)
```

#### 5. **Robust Loss Calculation**
```python
target_tensor = current_q_values.clone().detach()
loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
```

#### 6. **Debugging ve Monitoring**
- Her fonksiyona docstring eklendi
- Action seÃ§imlerinde print/log Ã§Ä±ktÄ±larÄ±
- NaN/Inf kontrolÃ¼ iÃ§in assert'ler
- Training progress monitoring

## ğŸ“ Dosya YapÄ±sÄ±

```
PaddleGame/
â”œâ”€â”€ dqn_ai.py              # GeliÅŸmiÅŸ DQN AI
â”œâ”€â”€ paddle_ai.py           # Basit Q-Learning AI
â”œâ”€â”€ paddle_game.py         # Ana oyun dosyasÄ±
â”œâ”€â”€ ai_test_untrained.py   # EÄŸitilmemiÅŸ model testi
â””â”€â”€ README.md              # Bu dosya
```

## ğŸ® KullanÄ±m

### EÄŸitim
```bash
cd PaddleGame
python paddle_game.py train
```

### Oyun Oynama
```bash
cd PaddleGame
python paddle_game.py
```

### EÄŸitilmemiÅŸ Model Testi
```bash
cd PaddleGame
python ai_test_untrained.py
```

## ğŸ”§ Teknik Detaylar

### State Vector (6 boyutlu)
1. `ball_y / WINDOW_HEIGHT` - Topun Y pozisyonu (normalize)
2. `paddle_y / WINDOW_HEIGHT` - Paddle'Ä±n Y pozisyonu (normalize)
3. `ball_direction_x / BALL_SPEED_X` - Topun X yÃ¶nÃ¼ (normalize)
4. `ball_direction_y / BALL_SPEED_Y` - Topun Y yÃ¶nÃ¼ (normalize)
5. `ball_velocity_x / 5.0` - Topun X hÄ±zÄ± (normalize) â­ **YENÄ°**
6. `paddle_height / PADDLE_HEIGHT` - Paddle yÃ¼ksekliÄŸi (normalize) â­ **YENÄ°**

### Reward Sistemi
- **Top vurma**: +2 puan
- **Merkeze yakÄ±n vurma**: +1 ek puan
- **Top kaÃ§Ä±rma**: -2 puan
- **Topa yakÄ±n olma**: +0.2 puan

### EÄŸitim Parametreleri
- **Learning Rate**: 0.001
- **Gamma (Discount Factor)**: 0.99
- **Epsilon Decay**: Logaritmik
- **Target Network Update**: Her 100 adÄ±m
- **Batch Size**: 32
- **Memory Size**: 10,000

## ğŸ§ª Test SenaryolarÄ±

### 1. EÄŸitilmemiÅŸ Model Testi
`ai_test_untrained.py` dosyasÄ± ile eÄŸitilmemiÅŸ modelin davranÄ±ÅŸÄ±nÄ± gÃ¶zlemleyebilirsiniz:
- Rastgele hareketler
- YÃ¼ksek epsilon deÄŸeri
- Frame sayÄ±sÄ± ve epsilon gÃ¶sterimi

### 2. EÄŸitim Ã–ncesi/SonrasÄ± KarÅŸÄ±laÅŸtÄ±rma
1. `ai_test_untrained.py` Ã§alÄ±ÅŸtÄ±r (eÄŸitilmemiÅŸ)
2. `paddle_game.py train` ile eÄŸitim yap
3. `paddle_game.py` ile eÄŸitilmiÅŸ modeli test et

## ğŸ“Š Monitoring ve Debugging

### Console Ã‡Ä±ktÄ±larÄ±
```
DQN AI initialized with state_size=6, action_size=3
Random action: 1 (epsilon: 0.950)
Greedy action: 0 (Q-values: [0.1, 0.3, 0.2])
Training step 1000, Loss: 0.0456, Epsilon: 0.368
Target network updated at step 100
```

### Debug Assertions
- State deÄŸerlerinde NaN/Inf kontrolÃ¼
- Reward tipi kontrolÃ¼
- Done deÄŸeri boolean kontrolÃ¼

## ğŸ¯ Performans Ä°yileÅŸtirmeleri

1. **CPU Optimizasyonu**: GPU yerine CPU kullanÄ±mÄ±
2. **KÃ¼Ã§Ã¼k Network**: 64-32-3 mimarisi
3. **Efficient Memory**: 10,000 deneyim limiti
4. **Frequent Updates**: Her 100 adÄ±mda target network gÃ¼ncelleme

## ğŸ”® Gelecek GeliÅŸtirmeler

- [ ] Prioritized Experience Replay
- [ ] Dueling DQN
- [ ] Multi-agent training
- [ ] Visual state representation
- [ ] Curriculum learning

## ğŸ“ Lisans

Bu proje eÄŸitim amaÃ§lÄ± geliÅŸtirilmiÅŸtir. 